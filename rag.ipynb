{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c27bfed-c7e1-4092-867f-c5bec106a352",
   "metadata": {},
   "source": [
    "## Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78837fb-8485-4203-ac09-c29d17abafe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install openai\n",
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf9bf5-f894-471b-a46d-3d229ab66c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from openai import OpenAI\n",
    "from openai.types.beta.threads.message_create_params import Attachment, AttachmentToolFileSearch\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea36f0-059c-444e-8fc5-3b4599455084",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270db65-340b-4ac9-a132-d5c58eb3af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'your_langchain_api_key'\n",
    "os.environ['OPENAI_API_KEY'] = 'your_openai_api_key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81d287-b8f7-4db2-8ec5-a6bcbebd80d7",
   "metadata": {},
   "source": [
    "## Important Note"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfad9dec-dff3-4150-b13e-f91875a9552e",
   "metadata": {},
   "source": [
    "We could feed the entire document to the LLM in this case (because the document is fairly small) and it would lead to an optimal answer. However, this is not scalable and would fall apart fairly quickly if we added approx. 3-5 documents (very likely when it comes to a whole course). That said, if a given course has very few notes, we could use a simple routing algorithm with this strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b424d8-01e2-457f-9ea6-d34eb03b803d",
   "metadata": {},
   "source": [
    "## LLM-Based PDF-to-Text Conversion (Quality > Traditional Approaches)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9edc3c46-e354-474e-8f95-dedd486e5c3e",
   "metadata": {},
   "source": [
    "Method: Feeds the PDF into an LLM, which returns a text copy accounting for formatting, LaTeX, diagrams, etc., with LLM-based error-checking in between to ensure the output is consistently correct. The output quality relative to PyPDF2, which is the usual approach, is much higher. It is slower though, but this should be okay as the course notes are loaded only a single time (by the lecturer when the course is created, for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abbe31-3be6-4dae-ba7f-ecb64e6d50b1",
   "metadata": {},
   "source": [
    "#### Split PDF Into Individual Pages (To Overcome Limited Context Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95480e-8c49-4f4d-8661-f0f5adaed2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"split_pages\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "rag_pdf = PdfReader('rag.pdf')\n",
    "\n",
    "for page in range(len(rag_pdf.pages)):\n",
    "    pdf_writer = PdfWriter()\n",
    "    pdf_writer.add_page(rag_pdf.pages[page])\n",
    "\n",
    "    output_filename = os.path.join(output_folder, f\"rag-{page + 1}.pdf\")\n",
    "\n",
    "    with open(output_filename, 'wb') as out:\n",
    "        pdf_writer.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0a3e1-87ff-4bf3-92c9-1fd03ebfc2df",
   "metadata": {},
   "source": [
    "# At This Point, Look at the Pages. Delete The Last One Manually As It Is Blank. It Behaves Weirdly For Some Reason - I Will Fix This Afterwards Through Extra Prompting for Blankness -> Boolean Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc4b89-fff7-40b1-82a4-f3ef5a9042ff",
   "metadata": {},
   "source": [
    "#### Checks If the LLM Response Indicates an Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad06f2-c0ff-4723-9bc9-10a199c61b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_error_message_with_chatgpt(response):\n",
    "    prompt = (\n",
    "        f\"Does the following text read like an error or technical issue? \"\n",
    "        f\"Reply with 'Yes' or 'No' only. (do not reply with anything else at all)\\n\\n{response}\"\n",
    "    )\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key = os.environ['OPENAI_API_KEY']\n",
    "    )\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content.strip().lower() == 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabd669-4337-4934-8886-0f88ac343949",
   "metadata": {},
   "source": [
    "#### Converts a Single Page into Text, Appending This to out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cc6e1-4a8f-4739-9885-4010ec24105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileToText(file_path):\n",
    "    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "    def get_assistant():\n",
    "        return client.beta.assistants.create(\n",
    "            model='gpt-4-1106-preview',\n",
    "            description='You are a data retrieval assistant.',\n",
    "            instructions=\"Return the COMPLETE content from the provided file, preserving all mathematical notation and formatting.\",\n",
    "            tools=[{\"type\": \"file_search\"}],\n",
    "            name='Maths PDF Reader',\n",
    "        )\n",
    "    \n",
    "    file = client.files.create(\n",
    "        file=open(file_path, 'rb'),\n",
    "        purpose='assistants'\n",
    "    )\n",
    "    \n",
    "    thread = client.beta.threads.create()\n",
    "    prompt = \"This is a mathematical document that may contain diagrams, LaTeX, and complex formatting. Please: 1. Extract ALL readable text content (Your goal is to convert the entire PDF to a text output that is as close a copy as possible) 2. Preserve mathematical notation where possible (convert LaTeX to an appropriate format) 3. Ignore diagrams and figures 4. Maintain section numbering and structure, EXCEPT the running headers and page numbers. Please ignore the running headers (which are in the top corners) and page numbers (also at the top, but the other corner). Remember, just read the file, don't add any extra comments.\"\n",
    "    \n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id = thread.id,\n",
    "        role='user',\n",
    "        content=prompt,\n",
    "        attachments=[Attachment(file_id=file.id, tools=[AttachmentToolFileSearch(type='file_search')])]\n",
    "    )\n",
    "    \n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=get_assistant().id,\n",
    "        timeout=300,\n",
    "        tools=[{\"type\": \"file_search\"}],\n",
    "    )\n",
    "    \n",
    "    if run.status != \"completed\":\n",
    "        print('Error Upon Reading... Retrying')\n",
    "        fileToText(file_path)\n",
    "        return\n",
    "    \n",
    "    messages_cursor = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "    messages = [message for message in messages_cursor]\n",
    "    \n",
    "    message = messages[0]\n",
    "    assert message.content[0].type == \"text\"\n",
    "    \n",
    "    res_txt = message.content[0].text.value + '\\n'\n",
    "\n",
    "    if is_error_message_with_chatgpt(res_txt):\n",
    "        print('Error Upon Reading... Retrying')\n",
    "        fileToText(file_path)\n",
    "        return\n",
    "\n",
    "    with open('out_2.txt', 'a', encoding='utf-8') as test_file:\n",
    "        test_file.write(res_txt)\n",
    "    \n",
    "    client.files.delete(file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df908cd9-2a0d-425b-a55c-eb6f4a6c7362",
   "metadata": {},
   "source": [
    "#### Iterates Through All Pages for Conversion (Slow, But This is One-Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875c0f4-53cb-4aad-8980-c70d3e5b9eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('split_pages')\n",
    "sorted_files = sorted(\n",
    "    files,\n",
    "    key=lambda x: int(re.search(r'(\\d+)', x).group(0)) if re.search(r'(\\d+)', x) else 0\n",
    ")\n",
    "\n",
    "for file in sorted_files:\n",
    "    file_path = os.path.join('split_pages', file)\n",
    "    fileToText(file_path)\n",
    "    print(f'{file_path} Processed')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33bff32a-4374-44de-ab91-b63a87f72421",
   "metadata": {},
   "source": [
    "Page 20 Led to Extra Content Generation\n",
    "- Might Want To Let User Delete Blank Pages\n",
    "- Or Ask the LLM Explicitly to Ignore Pages That Are Basically Blank\n",
    "\n",
    "Research Methods to Collect Latex Blocks After PyPDF2 (Regex Probably), and then use PyLaTeX\n",
    "- I Have Heard This is Suboptimal for Complex Equations, But it is Faster - Will Try Anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a07b9-7b73-440d-877b-9aa3801b93bc",
   "metadata": {},
   "source": [
    "# Final RAG Model (Not General, But There Are Some General Models Below and in the Ideas for the Future Section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb13d7-57aa-4425-b5dc-829dff6445c2",
   "metadata": {},
   "source": [
    "### Uses Multi-Query + RAG Tree-Based Parser + Routing Example + > Near-Maximum Cosine Similarity + Keeps History of Last 2 Messages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "011f32a9-3290-43e6-8a28-92f9be2892e3",
   "metadata": {},
   "source": [
    "Benefits of this Approach:\n",
    "- Prevents the retrieval of duplicate splits, so that the context window has sufficient variety (while reducing wastage)\n",
    "- Multi-query decomposes the problem into more fundamental ones (like an algorithm called Step Back), so that higher-level problems can be answered. This was useful to me when asking questions like 'what are the axioms of the reals', where the order axioms were in a separate part and not mentioned in an explicit, direct way.\n",
    "- Tree-based parser is the most efficient way to chunk this file while maintaining semantics\n",
    "- The routing was just to show how it could be used. We reduced the number of tokens used by a fair amount.\n",
    "- Cosine similarity is a common way to express how related vectors are (by their angles) - this was necessary for defining custom retrieval functions (because LangChain's retriever is highly susceptible to duplicates)\n",
    "- The history is just fun - you can ask 'what are the axioms of a field?', and then 'given the mathematical object i was asking about in my previous query, can i have an example of that object?' - and it will give you a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6eb664-c66b-4766-8e19-5fb3a420985a",
   "metadata": {},
   "source": [
    "### Go to the Final ChatBot Part to Interact With It (Just Before Ideas for the Future). Until That ChatBot Cell, Run Every Previous Cell In Order From the Beginning of the Whole Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdced54-da58-48f8-918a-68502ed8a481",
   "metadata": {},
   "source": [
    "#### Pre-Processing to Remove Running Headers + Page Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989efa4-ce21-46ba-b10a-087c3c1b2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_running_headers(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding = 'utf-8') as infile, open(output_file, \"w\", encoding = 'utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            if re.match(r'^\\s*CHAPTER\\s+\\d+', line):\n",
    "                continue\n",
    "            if re.match(r'^\\s*[A-Z0-9\\s\\.\\-]+$', line.strip()):\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()):\n",
    "                continue\n",
    "            outfile.write(line)\n",
    "\n",
    "remove_running_headers(\"out_2.txt\", \"cleaned_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a6829-7a24-4dc7-b1cf-a9eceb23fcbf",
   "metadata": {},
   "source": [
    "#### Creates a Tree Based on the Structure of Maths Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b5ad7-25f1-4bfc-9cc0-e82f7d1e59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathNode:\n",
    "    def __init__(self, content, node_type, identifier, parent=None, children=None):\n",
    "        self.content = content\n",
    "        self.node_type = node_type\n",
    "        self.identifier = identifier\n",
    "        self.parent = parent\n",
    "        self.children = children or []\n",
    "\n",
    "class MathDocumentParser:\n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'chapter': r'Chapter\\s+(\\d+)',\n",
    "            'section': r'(\\d+\\.\\d+)\\s+([^\\n]+)',\n",
    "            'subsection': r'(\\d+\\.\\d+\\.\\d+)\\s+([^\\n]+)',\n",
    "            'theorem': r'Theorem\\s+(\\d+)',\n",
    "            'lemma': r'Lemma\\s+(\\d+)',\n",
    "            'proposition': r'Proposition\\s+(\\d+)',\n",
    "            'definition': r'Definition\\s+(\\d+)',\n",
    "            'example': r'Example\\s+(\\d+)',\n",
    "            'proof': r'Proof(?:\\s+of\\s+(?:Theorem|Lemma|Proposition)\\s+\\d+)?'\n",
    "        }\n",
    "        self.root = None\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        self.root = MathNode(\"Document Root\", \"root\", \"root\")\n",
    "        current_chapter = None\n",
    "        current_section = None\n",
    "        current_subsection = None\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        current_content = []\n",
    "        current_node = None\n",
    "        \n",
    "        for line in lines:\n",
    "            matched = False\n",
    "            chapter_match = re.match(self.patterns['chapter'], line)\n",
    "            if chapter_match:\n",
    "                if current_node:\n",
    "                    current_node.content = '\\n'.join(current_content).strip()\n",
    "                current_chapter = MathNode(\n",
    "                    content=\"\",\n",
    "                    node_type=\"chapter\",\n",
    "                    identifier=f\"Chapter {chapter_match.group(1)}\",\n",
    "                    parent=self.root\n",
    "                )\n",
    "                self.root.children.append(current_chapter)\n",
    "                current_node = current_chapter\n",
    "                current_section = None\n",
    "                current_subsection = None\n",
    "                current_content = []\n",
    "                matched = True\n",
    "                continue\n",
    "                \n",
    "            section_match = re.match(self.patterns['section'], line)\n",
    "            if section_match and current_chapter:\n",
    "                if current_node:\n",
    "                    current_node.content = '\\n'.join(current_content).strip()\n",
    "                current_section = MathNode(\n",
    "                    content=\"\",\n",
    "                    node_type=\"section\",\n",
    "                    identifier=section_match.group(1),\n",
    "                    parent=current_chapter\n",
    "                )\n",
    "                current_chapter.children.append(current_section)\n",
    "                current_node = current_section\n",
    "                current_subsection = None\n",
    "                current_content = []\n",
    "                matched = True\n",
    "                continue\n",
    "\n",
    "            subsection_match = re.match(self.patterns['subsection'], line)\n",
    "            if subsection_match and current_section:\n",
    "                if current_node:\n",
    "                    current_node.content = '\\n'.join(current_content).strip()\n",
    "                current_subsection = MathNode(\n",
    "                    content=\"\",\n",
    "                    node_type=\"subsection\",\n",
    "                    identifier=subsection_match.group(1),\n",
    "                    parent=current_section\n",
    "                )\n",
    "                current_section.children.append(current_subsection)\n",
    "                current_node = current_subsection\n",
    "                current_content = []\n",
    "                matched = True\n",
    "                continue\n",
    "\n",
    "            for content_type, pattern in self.patterns.items():\n",
    "                if content_type in ['chapter', 'section', 'subsection']:\n",
    "                    continue\n",
    "                    \n",
    "                match = re.match(pattern, line)\n",
    "                if match:\n",
    "                    if current_node:\n",
    "                        current_node.content = '\\n'.join(current_content).strip()\n",
    "                    \n",
    "                    parent = current_subsection if current_subsection else current_section\n",
    "                    if not parent:\n",
    "                        continue\n",
    "                        \n",
    "                    new_node = MathNode(\n",
    "                        content=\"\",\n",
    "                        node_type=content_type,\n",
    "                        identifier=f\"{content_type.capitalize()} {match.group(1)}\" if content_type != 'proof' else \"Proof\",\n",
    "                        parent=parent\n",
    "                    )\n",
    "                    parent.children.append(new_node)\n",
    "                    current_node = new_node\n",
    "                    current_content = []\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched and current_node:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_node:\n",
    "            current_node.content = '\\n'.join(current_content).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ff0db-ab39-4208-891f-a6f020fbca0d",
   "metadata": {},
   "source": [
    "#### Both Prints a Summary of the Tree AND Creates the Tree Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2cd969-7950-4913-8463-d70748048481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, level = 0):\n",
    "    \"\"\"Print the tree in a readable format.\"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}├── {node.node_type}: {node.identifier}\")\n",
    "    if node.content:\n",
    "        content_preview = node.content.replace('\\n', ' ')[:50]\n",
    "        if len(node.content) > 50:\n",
    "            content_preview += \"...\"\n",
    "        print(f\"{indent}│   Content: {content_preview}\")\n",
    "    for child in node.children:\n",
    "        print_tree(child, level + 1)\n",
    "\n",
    "parser = MathDocumentParser()\n",
    "\n",
    "with open('cleaned_output.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "parser.parse(content)\n",
    "print(\"\\nDocument Structure:\")\n",
    "print_tree(parser.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d2512-8f38-4d9e-9d2d-c84f1aaf1123",
   "metadata": {},
   "source": [
    "#### Only the Leaves of the Tree are Document Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9822086-7863-4976-97a7-89513030d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_contents(node):\n",
    "    leaf_contents = []\n",
    "    \n",
    "    def collect_leaves(node_2):\n",
    "        if not node_2.children: \n",
    "            if node_2.content:\n",
    "                leaf_contents.append(Document(page_content = node_2.content))\n",
    "        else:\n",
    "            for child in node_2.children:\n",
    "                collect_leaves(child)\n",
    "    \n",
    "    collect_leaves(node)\n",
    "    return leaf_contents\n",
    "\n",
    "splits_leaves = get_leaf_contents(parser.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286d590-149f-4da7-99c9-633cad8b5729",
   "metadata": {},
   "source": [
    "#### This is Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f5e68-7c91-43a7-b70a-326a890282c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sub_queries(query):\n",
    "    sub_query_prompt = f\"\"\"Break down this mathematical query into smaller, related queries that would help build a complete answer.\n",
    "Query: {query}\n",
    "\n",
    "Let's generate specific sub-queries to find:\n",
    "1. The direct definition/axioms\n",
    "2. Related properties mentioned elsewhere\n",
    "3. Any additional conditions or properties that combine with these\n",
    "4. Any special cases or extensions\n",
    "\n",
    "I need specific sub-queries. For example, given a Query: List the axioms of the reals, you should return something similar to:\n",
    "What are the field axioms that the reals satisfy?,\n",
    "What are the ordering properties of the reals?,\n",
    "What additional properties are mentioned about the real numbers?,\n",
    "How are the reals defined in terms of fields and ordering?\n",
    "\n",
    "Do not try to answer the Query at all. Just create sub-queries (so questions that help to build up a larger picture. I'm using you for a RAG tool and this is multi-query)\n",
    "Also, when you generate sub-queries, do not use your own knowledge AT ALL. Just stick to sub-queries that generalize the query based on knowledge of English and NOT of maths. Remember, this is a RAG tool.\n",
    "To re-iterate, don't use your maths knowledge AT ALL. \n",
    "Just don't use your own knowledge of the Query AT ALL.\n",
    "\n",
    "Importantly, you are guiding this strategy. So as the master of this RAG tool, you should 3 choose sub-queries that will be able to gather information from disparate parts of the input file.\n",
    "\n",
    "Just don't use your maths knowledge AT ALL. \n",
    "Just don't use your own knowledge of the Query AT ALL.\n",
    "\n",
    "\n",
    "Generate 3 sub-queries:\"\"\"\n",
    "\n",
    "    # Can probably optimise the prompt naturally\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key = os.environ['OPENAI_API_KEY']\n",
    "    )\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sub_query_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    qs = chat_completion.choices[0].message.content\n",
    "    return [q.strip().split('. ', 1)[1] if '. ' in q else q.strip() for q in qs.split('\\n')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097558c2-4677-45ea-950f-afd5b51214b9",
   "metadata": {},
   "source": [
    "#### Example of Routing (This is a Proof-of-Concept More Than Anything). Just Determines Whether a Query Asks for a Proof or Not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2465f43-2992-4a56-a61f-c48b57cdfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_math_query(query):\n",
    "    \n",
    "    classification_prompt = f\"\"\"Determine if this mathematical query is asking for a proof or something else.\n",
    "\n",
    "    Think step by step:\n",
    "    1. Is this asking for a proof/demonstration of why something is true?\n",
    "    2. Or is it asking for something else (definition, explanation, etc.)?\n",
    "    \n",
    "    Query: {query}\n",
    "    \n",
    "    Return ONLY 'proof' or 'other'.\"\"\"\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key = os.environ['OPENAI_API_KEY']\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": classification_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4-1106-preview\"\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38fd27-cffb-46ed-9513-1e569cb4d87e",
   "metadata": {},
   "source": [
    "#### Retrieval Algorithm for Non-Proofs (Again, Arbitrary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eea75ae-97b5-400e-b46f-9c4803a90a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_documents_without_neighbors(documents, query):\n",
    "    query_embedding = embd.embed_query(query)\n",
    "    cos_sim = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    doc_with_scores = [(doc, cos_sim[i]) for i, doc in enumerate(documents)]\n",
    "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    threshold = math.floor((sorted_docs[0][1] - 0.01) / 0.05) * 0.05 # This works, but can vary it depending on the query\n",
    "                                                            # Or do some routing based on the type of query (but this is decent)\n",
    "    selected_docs = []\n",
    "    for doc, csm in sorted_docs:\n",
    "        if csm > threshold:\n",
    "            selected_docs.append(doc)\n",
    "        else:\n",
    "            break\n",
    "    return selected_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc96ee1-4618-4cc7-8da3-1607e3d389f3",
   "metadata": {},
   "source": [
    "#### Retrieval Algorithm for Proofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbdef2-a0d1-41ac-8596-2f712128de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_distinct(documents, query, n):\n",
    "    query_embedding = embd.embed_query(query)\n",
    "    cos_sim = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    doc_with_scores = [(doc, cos_sim[i]) for i, doc in enumerate(documents)]\n",
    "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in sorted_docs[:n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91700a61-9f06-4a9e-bc7a-11545d8616c2",
   "metadata": {},
   "source": [
    "#### Doing This Here Makes Some Other Functions More Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7498e55-95f1-47fe-a772-38343560bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = OpenAIEmbeddings()\n",
    "doc_embeddings = [embd.embed_query(doc.page_content) for doc in splits_leaves]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af7bd9c-5c28-49c5-835a-52cf20ab4ffa",
   "metadata": {},
   "source": [
    "#### Final ChatBot (Uses Multi-Query + RAG Tree-Based Parser + Routing Example + > Near-Maximum Cosine Similarity + Keeps History of Last 2 Messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a97ac-dd45-463a-8858-7e3b984f359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "    {context}\n",
    "    \n",
    "    Question To Answer Now: {question}\n",
    "\n",
    "    Previous Question:Response Pairs in Order from Least Recent to Most Recent (You are a ChatBot, so these are the previous Questions asked of you, and the Responses you gave to each):{q_a}\n",
    "    \n",
    "    Mathematical Response:\n",
    "    [Response here, using formal mathematical notation where appropriate]\n",
    "    \n",
    "    Important Notes:\n",
    "    - All statements are made strictly based on the provided context\n",
    "    - Any mathematical notation used follows directly from the source material\n",
    "    - No additional mathematical facts or properties are assumed beyond what's given\n",
    "    - Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "    - Do not miss out on critical information. Make sure to include this.\n",
    "    - You may or may not need to incorporate the previous Questions and Responses. Do this when you think it is suitable.\n",
    "    \n",
    "    [If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "    \"\"\"\n",
    "\n",
    "q_a_list = []\n",
    "\n",
    "while True:\n",
    "    distinct_docs = []\n",
    "    query = input('Query: ')\n",
    "    q_a = 'No Previous Questions and Responses'\n",
    "    if len(q_a_list) != 0:\n",
    "        q_a = '\\n' + ''.join(q_a_list)\n",
    "    query_template = f\"\"\"The Current Query is: {query}.\n",
    "    The previous Questions and Responses in order from Least Recent to Most Recent (You are a helper tool to a Chatbot, so these are the previous Questions asked of it, and the Responses it gave to each):{q_a}\n",
    "    \"\"\"\n",
    "    query_type = classify_math_query(query)\n",
    "    if query_type == 'other':\n",
    "        sub_queries = generate_sub_queries(query_template)\n",
    "        while len(sub_queries) != 3:\n",
    "            sub_queries = generate_sub_queries(query_template)\n",
    "        \n",
    "        for sq in [query] + sub_queries:\n",
    "            distinct_docs.append(get_sorted_documents_without_neighbors(splits_leaves, sq))\n",
    "            \n",
    "        seen_content = set()\n",
    "        unique_docs = []\n",
    "        for doc in [item for sublist in distinct_docs for item in sublist]:\n",
    "            if doc.page_content not in seen_content:\n",
    "                seen_content.add(doc.page_content)\n",
    "                unique_docs.append(doc)\n",
    "    else:\n",
    "        unique_docs = get_top_n_distinct(splits_leaves, query, 1)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "    \n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "        q_a = 'No Previous Questions and Responses'\n",
    "        if len(q_a_list) != 0:\n",
    "            q_a = '\\n' + ''.join(q_a_list)\n",
    "        response = rag_chain.invoke({\"context\": unique_docs,\"question\": query, \"q_a\": q_a})\n",
    "        file.write(response)\n",
    "        if len(q_a_list) < 2:\n",
    "            q_a_list.append('Question: ' + query + ', Response: ' + response + '\\n')\n",
    "        else:\n",
    "            q_a_list.pop(0)\n",
    "            q_a_list.append('Question: ' + query + ', Response: ' + response + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ed73c-84c1-4f76-ae48-f48e9d9ad26e",
   "metadata": {},
   "source": [
    "# If you go to final.txt, your output is there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e42f7-90b8-453c-910a-970e2deb5f33",
   "metadata": {},
   "source": [
    "## Ideas for the Future"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89c01c49-45eb-4b79-b4f1-838737e98654",
   "metadata": {},
   "source": [
    "- Routing is definitely necessary to limit the size of the context. With the custom retrieval approach, it is challenging to find a  threshold formula that works and is universal. For example, we can route proof-type queries into a custom retrieval function (or the default retriever) that returns 1 or 2 splits (because proofs are self-contained in our tree). Right now, we're including slightly too many documents and this is the most obvious way to optimize. (In this case, we could just explicitly pattern-match instead of LLM routing. If we cover enough cases, this is more efficient. But LLM routing does work well in a generalisable way, which is what we're after)\n",
    "\n",
    "- The above is quite tedious. But, if we build a large-enough backlog, it will be eventually be applicable to different types of notes. For example, chemistry notes have definitions + there are only so many reasonable classes of questions a user can ask about.\n",
    "\n",
    "- With regards to routing, might be worth giving the LLM a summary of our tree structure (similar to what is displayed by print_tree) and our query and asking it for a recommended number of nodes to supply to a retriever, or recommended parameters to the custom retrieval function (better) (this is experimental and probably not final, there is a cost/latency issue with the extra call) \n",
    "\n",
    "- Also, we can write a custom retrieval function that takes in as a parameter how many documents are to be retrieved as opposed to the threshold value\n",
    "\n",
    "- I suspect separating queries into multiple queries when the word 'and' is used (though I can think of at least 2 cases we'd have to cover (independent clause and independent clause) and (i want to do x and y)) may be potentially important (as otherwise, if they really are independent, the amount of documents pertaining to them is cut in at least half - actually, this is only true if we use the top N documents, which at the moment we do not use because we use a threshold value, so we are probably fine by some kind of compensating error with the cosine similarity function. Point is, should probably keep this in mind for later if we do write routing functions.\n",
    "\n",
    "- Need to optimize prompt engineering for space as well. The prompts lead to high-quality outputs, but I believe they can be made more efficient.\n",
    "\n",
    "- If we optimize for space (token usage) sufficiently, we could switch over to GPT 3.5 without reaching its context limit. This is cheaper. We could also just do this dynamically now.\n",
    "\n",
    "- We can also considering being restrictive when retrieving slices due to the ChatBot's history feature - by this, I mean we can minimize the number of slices to optimize performance as if there is insufficient information, the user can prompt the chatbot to fetch more information. This is challenging as it seems susceptible to duplicates, but perhaps we can dynamically remove documents from the current list of documents, for example.\n",
    "\n",
    "- I think this as a starting project led to high-quality outputs (even without any ChatBot history feature, just based on isolated inputs) with a clear direction for improvements in efficiency, but it was largely because we knew how to split. One idea is to use an LLM to ask for delimiters or ideally for section types, and then create a tree-parsing algorithm based on that (this therefore generalises to different documents). However, if for example we have a piece of literature, etc., a standard RecursiveCharacterTextSplitter is probably fine (and better). Once again, further research should be done to be exhaustive, but routing is probably a good way to optimize for performance and quality here (particularly as this only occurs at the initial file-uploading stage).\n",
    "\n",
    "- Realistically, we'd either need a central server or the user could enter their own api keys. Both work - the second one is better cost-wise but there are obvious disadvantages in user-friendliness and perhaps with performance depending on their device. Yeah, I'm leaning towards the server surely (?), but we'll see. \n",
    "\n",
    "- There are some research topics I came across that I'd like to read further into as far as optimisation goes + generalising for other document types for the actual final app: Query Translation (RAG Fusion, Decomposition, Step Back, HyDE), Indexing (Multi-Representation, RAPTOR, ColBERT), Active RAG, Adaptive RAG.\n",
    "\n",
    "- Some Potential General Solutions To Explore: - Adaptive RAG + (Duplicate Removal + Spread) (Most General Solution)\n",
    "                                               - Adaptive RAG + Re-Introducing Math Delimiters\n",
    "                                               - Adaptive RAG Through Continuous Conversation\n",
    "  These should be capable of high-level \"reasoning\", but they are fairly expensive\n",
    "\n",
    "- Lots of routing can take place at most decision points that occur when loading files. That might lead to optimal parameters (like the amount of overlap and chunk size for a given RecursiveCharacterTextSplitter), but overdoing it is probably not scalable, so caution must be applied (if it does occur at the beginning, it might be okay barring cost, etc.).\n",
    "\n",
    "- I am very happy to talk to professors in my own time to figure out the best approach.\n",
    "\n",
    "- I think it would be a good idea to be able to upload lectures as well (tend to have subtitle files), which would encourage a summary/bullet-list feature (this would be done via an LLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ee01e-d94f-4cae-bed2-8c7849e3042c",
   "metadata": {},
   "source": [
    "# What I Did to Get to The Above Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594cec2-9771-4386-a21b-535ef49209f1",
   "metadata": {},
   "source": [
    "## Simple RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af90777-951a-496f-9274-207bd630770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.txt', 'r', encoding = 'utf-8') as file:\n",
    "    text = file.read()\n",
    "doc = Document(page_content = text)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb0b61-bd2f-4a59-9c38-ffb090557eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074c708-2c19-4011-9cec-5ab3682b1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Mathematical Response:\n",
    "[Response here, using formal mathematical notation where appropriate]\n",
    "\n",
    "Important Notes:\n",
    "- All statements are made strictly based on the provided context\n",
    "- Any mathematical notation used follows directly from the source material\n",
    "- No additional mathematical facts or properties are assumed beyond what's given\n",
    "- Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "- Do not miss out on critical information. Make sure to include this.\n",
    "\n",
    "[If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = 'What are the axioms of a field?'\n",
    "with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "    file.write(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2538fecf-0f2e-43d9-b12d-b3c93646db55",
   "metadata": {},
   "source": [
    "'What are the axioms of a field?': Missed first three axioms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8961b0b6-6555-41a6-98e8-a6691c5e3f46",
   "metadata": {},
   "source": [
    "- Consistently misses out on critical information even in the locality\n",
    "- LangSmith API indicates duplication of retrieved documents\n",
    "- Might want to try splitting at more accurate, albeit hard-coded points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8520c1b-1b1c-4f7f-9796-8c2e4b497a89",
   "metadata": {},
   "source": [
    "## Simple RAG Model + Math Delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8daba-251d-44ad-9b78-47d16dd6f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.txt', 'r', encoding = 'utf-8') as file:\n",
    "    text = file.read()\n",
    "doc = Document(page_content = text)\n",
    "\n",
    "math_delimiters = [\n",
    "    \"\\nTheorem\", \"\\nLemma\", \"\\nDefinition\", \"\\nProof\",\n",
    "    \"\\nExample\", \"\\nProposition\", \"\\nCorollary\",\n",
    "    \"\\n\\n\", \"\\n\", \" \", \"\"\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,\n",
    "    separators=math_delimiters\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea2f0b-c918-4198-aa49-b6b830bdac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4e539-8830-40db-8290-adc529f125d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Mathematical Response:\n",
    "[Response here, using formal mathematical notation where appropriate]\n",
    "\n",
    "Important Notes:\n",
    "- All statements are made strictly based on the provided context\n",
    "- Any mathematical notation used follows directly from the source material\n",
    "- No additional mathematical facts or properties are assumed beyond what's given\n",
    "- Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "- Do not miss out on critical information. Make sure to include this.\n",
    "\n",
    "[If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = 'Can you define the reals using the decimal expansion definition?'\n",
    "with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "    file.write(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "243b424c-1f94-4ba1-b9df-bd6919fcb400",
   "metadata": {},
   "source": [
    "'What are the axioms of a field?': Good\n",
    "'Prove that the square root of 2 is irrational': Good\n",
    "'What is a Markov Chain?': Good (Does give an answer, but mentions this is not in the notes - can easily tweak to refuse completely)\n",
    "'Can you list all definitions in the context?' - Misses out on some definitions (otherwise, formatting is good)\n",
    "'Can you define the reals using the decimal expansion definition?' - Good (says yes, but does not expand because it's not in the notes - once again\n",
    " can tweak if required - should try)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc406062-0afc-4a4b-85d8-88c516b9ba63",
   "metadata": {},
   "source": [
    "- Could consider a memory-based approach, where the LLM remembers previous question-answer pairs in a continuous conversation\n",
    "- Also, I don't like hard-coding math_delimiters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a7fc5-b105-4e38-9897-90eb55f93bee",
   "metadata": {},
   "source": [
    "## Removing Math Delimiters & Forcing Uniqueness (Custom Retrieval)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "869a94b0-ff28-4ab5-b861-e6f19f5df60c",
   "metadata": {},
   "source": [
    "The most intuitive way I can describe this is that it 'shoots and spreads'. The retrieval algorithm supplied by LangChain is highly susceptible to exact duplicates, so this is a custom retrieval algorithm that finds distinct-enough splits most \"related\" to the query (arbitrarily, top 3 in this case). It then adds the neighbouring splits as well (without creating duplicates) so that the surrounding context is also provided. This is suitable as it generally provides all necessary information for non-exhaustive queries without having to hard-code delimiters, so it can be used generally for other non-math subjects (though we can write an LLM query to identify appropriate delimiters, but that has an added cost and we shouldn't overdo it - should consider it later). Further, the guaranteed distinctness enables diversity (so we get extra context that is still relevant but not identical - e.g. previous definitions, proofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab8d2e-7a55-4d7d-b1a9-f065f32ba7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.txt', 'r', encoding = 'utf-8') as file:\n",
    "    text = file.read()\n",
    "doc = Document(page_content = text)\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbf84b-38b1-4258-bcaf-35a84d400bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_documents_with_neighbors(documents, query, num_results=3):\n",
    "    embd = OpenAIEmbeddings()\n",
    "    doc_embeddings = [embd.embed_query(doc.page_content) for doc in documents]\n",
    "    query_embedding = embd.embed_query(query)\n",
    "    cos_sim = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    doc_with_scores = [(doc, cos_sim[i], i) for i, doc in enumerate(documents)]\n",
    "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_docs = sorted_docs[:num_results]\n",
    "    selected_docs = []\n",
    "    added_indices = set()\n",
    "    for doc, _, idx in top_docs:\n",
    "        if idx not in added_indices:\n",
    "            selected_docs.append(doc)\n",
    "            added_indices.add(idx)\n",
    "        if idx > 0 and (idx - 1) not in added_indices:\n",
    "            selected_docs.append(documents[idx - 1])\n",
    "            added_indices.add(idx - 1)\n",
    "        if idx < len(documents) - 1 and (idx + 1) not in added_indices:\n",
    "            selected_docs.append(documents[idx + 1])\n",
    "            added_indices.add(idx + 1)\n",
    "    return selected_docs\n",
    "\n",
    "query = 'Can you list all definitions in the context?'\n",
    "distinct_docs = get_sorted_documents_with_neighbors(splits, query, num_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56e7ed-fcbc-4ed0-bd0d-34b23bfc48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Mathematical Response:\n",
    "[Response here, using formal mathematical notation where appropriate]\n",
    "\n",
    "Important Notes:\n",
    "- All statements are made strictly based on the provided context\n",
    "- Any mathematical notation used follows directly from the source material\n",
    "- No additional mathematical facts or properties are assumed beyond what's given\n",
    "- Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "- Do not miss out on critical information. Make sure to include this.\n",
    "\n",
    "[If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "    response = rag_chain.invoke({\"context\": distinct_docs,\"question\": query})\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3844680-246d-49a8-8efd-e7697d44c234",
   "metadata": {},
   "source": [
    "'What are the axioms of a field?': Good\n",
    "'Prove that the square root of 2 is irrational': Good\n",
    "'Can you list all definitions in the context?' - Misses out on some definitions (otherwise, formatting is good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7fd25-b93e-4f50-b08d-47ae0c408402",
   "metadata": {},
   "source": [
    "## Math Delimiters + >80% Cosine Similarity (Does Not Work, Chunks Too Large -> Dilution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5996a12-e280-426d-b989-e404e5cb0828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.txt', 'r', encoding = 'utf-8') as file:\n",
    "    text = file.read()\n",
    "doc = Document(page_content = text)\n",
    "\n",
    "math_delimiters = [\n",
    "    \"\\nTheorem\", \"\\nLemma\", \"\\nDefinition\", \"\\nProof\",\n",
    "    \"\\nExample\", \"\\nProposition\", \"\\nCorollary\",\n",
    "    \"\\n\\n\", \"\\n\", \" \", \"\"\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,\n",
    "    separators=math_delimiters\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696b004-38e9-43f1-a001-9eb4568d70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_documents_without_neighbors(documents, query):\n",
    "    embd = OpenAIEmbeddings()\n",
    "    doc_embeddings = [embd.embed_query(doc.page_content) for doc in documents]\n",
    "    query_embedding = embd.embed_query(query)\n",
    "    cos_sim = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    doc_with_scores = [(doc, cos_sim[i]) for i, doc in enumerate(documents)]\n",
    "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    selected_docs = []\n",
    "    for doc, csm in sorted_docs:\n",
    "        if csm > 0.8:\n",
    "            selected_docs.append(doc)\n",
    "    return selected_docs\n",
    "\n",
    "query = 'Can you list all definitions in the context?'\n",
    "distinct_docs = get_sorted_documents_without_neighbors(splits, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60cfaca-d58d-4694-b2a6-ef279e85c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Mathematical Response:\n",
    "[Response here, using formal mathematical notation where appropriate]\n",
    "\n",
    "Important Notes:\n",
    "- All statements are made strictly based on the provided context\n",
    "- Any mathematical notation used follows directly from the source material\n",
    "- No additional mathematical facts or properties are assumed beyond what's given\n",
    "- Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "- Do not miss out on critical information. Make sure to include this.\n",
    "\n",
    "[If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "    response = rag_chain.invoke({\"context\": distinct_docs,\"question\": query})\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a531179-2954-4125-9eb2-17ffb020b651",
   "metadata": {},
   "source": [
    "## RAG Tree-Based Parser + > Near-Maximum Cosine Similarity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a1177e7-797e-4c34-ae63-758248175c14",
   "metadata": {},
   "source": [
    "Should lead to smaller contexts + Don't have to remove uniques (this actually motivates going back to the built-in retriever. I'll try\n",
    "that after this). Also, though it looks big, it is fast and the actual tree + node creation is only done once (again, at the beginning\n",
    "by the person that uploads the course notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d921d-5955-4996-ba24-2cad665d94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "class MathNode:\n",
    "    def __init__(self, content, node_type, identifier, parent=None, children=None):\n",
    "        self.content = content\n",
    "        self.node_type = node_type\n",
    "        self.identifier = identifier\n",
    "        self.parent = parent\n",
    "        self.children = children or []\n",
    "\n",
    "class MathDocumentParser:\n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'chapter': r'Chapter\\s+(\\d+)',\n",
    "            'section': r'(\\d+\\.\\d+)\\s+([^\\n]+)',\n",
    "            'subsection': r'(\\d+\\.\\d+\\.\\d+)\\s+([^\\n]+)',\n",
    "            'theorem': r'Theorem\\s+(\\d+)',\n",
    "            'lemma': r'Lemma\\s+(\\d+)',\n",
    "            'proposition': r'Proposition\\s+(\\d+)',\n",
    "            'definition': r'Definition\\s+(\\d+)',\n",
    "            'example': r'Example\\s+(\\d+)',\n",
    "            'proof': r'Proof(?:\\s+of\\s+(?:Theorem|Lemma|Proposition)\\s+\\d+)?'\n",
    "        }\n",
    "        self.root = None\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        self.root = MathNode(\"Document Root\", \"root\", \"root\")\n",
    "        current_chapter = None\n",
    "        current_section = None\n",
    "        current_subsection = None\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        current_content = []\n",
    "        current_node = None\n",
    "        \n",
    "        for line in lines:\n",
    "            matched = False\n",
    "            chapter_match = re.match(self.patterns['chapter'], line)\n",
    "            if chapter_match:\n",
    "                if current_node:\n",
    "                    current_node.content = '\\n'.join(current_content).strip()\n",
    "                current_chapter = MathNode(\n",
    "                    content=\"\",\n",
    "                    node_type=\"chapter\",\n",
    "                    identifier=f\"Chapter {chapter_match.group(1)}\",\n",
    "                    parent=self.root\n",
    "                )\n",
    "                self.root.children.append(current_chapter)\n",
    "                current_node = current_chapter\n",
    "                current_section = None\n",
    "                current_subsection = None\n",
    "                current_content = []\n",
    "                matched = True\n",
    "                continue\n",
    "                \n",
    "            section_match = re.match(self.patterns['section'], line)\n",
    "            if section_match and current_chapter:\n",
    "                if current_node:\n",
    "                    current_node.content = '\\n'.join(current_content).strip()\n",
    "                current_section = MathNode(\n",
    "                    content=\"\",\n",
    "                    node_type=\"section\",\n",
    "                    identifier=section_match.group(1),\n",
    "                    parent=current_chapter\n",
    "                )\n",
    "                current_chapter.children.append(current_section)\n",
    "                current_node = current_section\n",
    "                current_subsection = None\n",
    "                current_content = []\n",
    "                matched = True\n",
    "                continue\n",
    "\n",
    "            subsection_match = re.match(self.patterns['subsection'], line)\n",
    "            if subsection_match and current_section:\n",
    "                if current_node:\n",
    "                    current_node.content = '\\n'.join(current_content).strip()\n",
    "                current_subsection = MathNode(\n",
    "                    content=\"\",\n",
    "                    node_type=\"subsection\",\n",
    "                    identifier=subsection_match.group(1),\n",
    "                    parent=current_section\n",
    "                )\n",
    "                current_section.children.append(current_subsection)\n",
    "                current_node = current_subsection\n",
    "                current_content = []\n",
    "                matched = True\n",
    "                continue\n",
    "\n",
    "            for content_type, pattern in self.patterns.items():\n",
    "                if content_type in ['chapter', 'section', 'subsection']:\n",
    "                    continue\n",
    "                    \n",
    "                match = re.match(pattern, line)\n",
    "                if match:\n",
    "                    if current_node:\n",
    "                        current_node.content = '\\n'.join(current_content).strip()\n",
    "                    \n",
    "                    parent = current_subsection if current_subsection else current_section\n",
    "                    if not parent:\n",
    "                        continue\n",
    "                        \n",
    "                    new_node = MathNode(\n",
    "                        content=\"\",\n",
    "                        node_type=content_type,\n",
    "                        identifier=f\"{content_type.capitalize()} {match.group(1)}\" if content_type != 'proof' else \"Proof\",\n",
    "                        parent=parent\n",
    "                    )\n",
    "                    parent.children.append(new_node)\n",
    "                    current_node = new_node\n",
    "                    current_content = []\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched and current_node:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_node:\n",
    "            current_node.content = '\\n'.join(current_content).strip()\n",
    "\n",
    "def print_tree(node: MathNode, level: int = 0):\n",
    "    \"\"\"Print the tree in a readable format.\"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}├── {node.node_type}: {node.identifier}\")\n",
    "    if node.content:\n",
    "        content_preview = node.content.replace('\\n', ' ')[:50]\n",
    "        if len(node.content) > 50:\n",
    "            content_preview += \"...\"\n",
    "        print(f\"{indent}│   Content: {content_preview}\")\n",
    "    for child in node.children:\n",
    "        print_tree(child, level + 1)\n",
    "\n",
    "parser = MathDocumentParser()\n",
    "\n",
    "with open('cleaned_output.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "parser.parse(content)\n",
    "print(\"\\nDocument Structure:\")\n",
    "print_tree(parser.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65507087-0da2-4bf2-86b7-3704067e6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_contents(node):\n",
    "    leaf_contents = []\n",
    "    \n",
    "    def collect_leaves(node_2):\n",
    "        if not node_2.children: \n",
    "            if node_2.content:\n",
    "                leaf_contents.append(Document(page_content = node_2.content))\n",
    "        else:\n",
    "            for child in node_2.children:\n",
    "                collect_leaves(child)\n",
    "    \n",
    "    collect_leaves(node)\n",
    "    return leaf_contents\n",
    "\n",
    "splits_leaves = get_leaf_contents(parser.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc79465-d9ec-4e62-b57e-236e85c8e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_documents_without_neighbors(documents, query):\n",
    "    embd = OpenAIEmbeddings()\n",
    "    doc_embeddings = [embd.embed_query(doc.page_content) for doc in documents]\n",
    "    query_embedding = embd.embed_query(query)\n",
    "    cos_sim = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    doc_with_scores = [(doc, cos_sim[i]) for i, doc in enumerate(documents)]\n",
    "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    threshold = math.floor(sorted_docs[0][1] / 0.05) * 0.05 # Might be able to answer more complex queries with > 0.05 at a greater cost\n",
    "                                                            # Or do some routing based on the type of query (but this is decent)\n",
    "    selected_docs = []\n",
    "    for doc, csm in sorted_docs:\n",
    "        if csm > threshold:\n",
    "            selected_docs.append(doc)\n",
    "    return selected_docs\n",
    "\n",
    "query = 'list the axioms of the reals'\n",
    "distinct_docs = get_sorted_documents_without_neighbors(splits_leaves, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031a972-3fc1-447b-b7c9-82c4a0a0fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Mathematical Response:\n",
    "[Response here, using formal mathematical notation where appropriate]\n",
    "\n",
    "Important Notes:\n",
    "- All statements are made strictly based on the provided context\n",
    "- Any mathematical notation used follows directly from the source material\n",
    "- No additional mathematical facts or properties are assumed beyond what's given\n",
    "- Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "- Do not miss out on critical information. Make sure to include this.\n",
    "\n",
    "[If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "    response = rag_chain.invoke({\"context\": distinct_docs,\"question\": query})\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3af7adc9-fce2-4826-a218-2eb5b544222a",
   "metadata": {},
   "source": [
    "It doesn't really answer 'list the axioms of the reals' correctly. It gets the field axioms, but misses out on the axioms of an ordered field. I am going to re-write the initial PDF-reading function to ignore the corner headings and page number on each page (maybe that reduces distractions).\n",
    "\n",
    "That didn't really work. Will leave the automatic filter for later (probably just better prompt engineering - e.g. removes lines from the text file that 'look like' running headers and page numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db778a1-19e7-46d6-b52e-b510eb213305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just doing it manually for now\n",
    "def remove_running_headers(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding = 'utf-8') as infile, open(output_file, \"w\", encoding = 'utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            if re.match(r'^\\s*CHAPTER\\s+\\d+', line):\n",
    "                continue\n",
    "            if re.match(r'^\\s*[A-Z0-9\\s\\.\\-]+$', line.strip()):\n",
    "                continue\n",
    "            if re.match(r'^\\s*\\d+\\s*$', line.strip()):\n",
    "                continue\n",
    "            outfile.write(line)\n",
    "\n",
    "remove_running_headers(\"out_2.txt\", \"cleaned_output.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fef0875-bb46-4b43-864b-fbc3055fde6d",
   "metadata": {},
   "source": [
    "I'll rerun the above RAG chain. Still didn't work (I made sure to change the input .txt file). I think the best approach to gain an overarching view is either multi-query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc1d20-d65f-4cb8-9d87-c23587a04587",
   "metadata": {},
   "source": [
    "## Multi-Query + RAG Tree-Based Parser + > Near-Maximum Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79fcc62-5b03-4e2d-8832-99fa260f2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sub_queries(query):\n",
    "    sub_query_prompt = f\"\"\"Break down this mathematical query into smaller, related queries that would help build a complete answer.\n",
    "Query: {query}\n",
    "\n",
    "Let's generate specific sub-queries to find:\n",
    "1. The direct definition/axioms\n",
    "2. Related properties mentioned elsewhere\n",
    "3. Any additional conditions or properties that combine with these\n",
    "4. Any special cases or extensions\n",
    "\n",
    "I need specific sub-queries. For example, given a Query: List the axioms of the reals, you should return something similar to:\n",
    "What are the field axioms that the reals satisfy?,\n",
    "What are the ordering properties of the reals?,\n",
    "What additional properties are mentioned about the real numbers?,\n",
    "How are the reals defined in terms of fields and ordering?\n",
    "\n",
    "Do not try to answer the Query at all. Just create sub-queries (so questions that help to build up a larger picture. I'm using you for a RAG tool and this is multi-query)\n",
    "Also, when you generate sub-queries, do not use your own knowledge AT ALL. Just stick to sub-queries that generalize the query based on knowledge of English and NOT of maths. Remember, this is a RAG tool.\n",
    "To re-iterate, don't use your maths knowledge AT ALL. \n",
    "Just don't use your own knowledge of the Query AT ALL.\n",
    "\n",
    "Importantly, you are guiding this strategy. So as the master of this RAG tool, you should 3 choose sub-queries that will be able to gather information from disparate parts of the input file.\n",
    "\n",
    "Just don't use your maths knowledge AT ALL. \n",
    "Just don't use your own knowledge of the Query AT ALL.\n",
    "\n",
    "\n",
    "Generate 3 sub-queries:\"\"\"\n",
    "\n",
    "    # Can probably optimise the prompt naturally\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key = os.environ['OPENAI_API_KEY']\n",
    "    )\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sub_query_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    qs = chat_completion.choices[0].message.content\n",
    "    return [q.strip().split('. ', 1)[1] if '. ' in q else q.strip() for q in qs.split('\\n')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe544d-16da-40e9-abfa-3509129af94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = OpenAIEmbeddings()\n",
    "doc_embeddings = [embd.embed_query(doc.page_content) for doc in splits_leaves]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031f8bf-56a0-4d6e-8644-21462bc52d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_docs = []\n",
    "\n",
    "def get_sorted_documents_without_neighbors(documents, query):\n",
    "    query_embedding = embd.embed_query(query)\n",
    "    cos_sim = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    doc_with_scores = [(doc, cos_sim[i]) for i, doc in enumerate(documents)]\n",
    "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    threshold = math.floor((sorted_docs[0][1] - 0.01) / 0.05) * 0.05 # This works, but can vary it depending on the query\n",
    "                                                            # Or do some routing based on the type of query (but this is decent)\n",
    "    selected_docs = []\n",
    "    for doc, csm in sorted_docs:\n",
    "        if csm > threshold:\n",
    "            selected_docs.append(doc)\n",
    "        else:\n",
    "            break\n",
    "    return selected_docs\n",
    "\n",
    "query = 'prove the square root of 2 is irrational'\n",
    "sub_queries = generate_sub_queries(query)\n",
    "while len(sub_queries) != 3:\n",
    "    sub_queries = generate_sub_queries(query)\n",
    "\n",
    "for sq in [query] + sub_queries:\n",
    "    distinct_docs.append(get_sorted_documents_without_neighbors(splits_leaves, sq))\n",
    "    print(\"Processed a sub-query\")\n",
    "    \n",
    "seen_content = set()\n",
    "unique_docs = []\n",
    "for doc in [item for sublist in distinct_docs for item in sublist]:\n",
    "    if doc.page_content not in seen_content:\n",
    "        seen_content.add(doc.page_content)\n",
    "        unique_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150cbb7d-6af6-4307-876d-7de65628b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Mathematical Response:\n",
    "[Response here, using formal mathematical notation where appropriate]\n",
    "\n",
    "Important Notes:\n",
    "- All statements are made strictly based on the provided context\n",
    "- Any mathematical notation used follows directly from the source material\n",
    "- No additional mathematical facts or properties are assumed beyond what's given\n",
    "- Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "- Do not miss out on critical information. Make sure to include this.\n",
    "\n",
    "[If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "    response = rag_chain.invoke({\"context\": unique_docs,\"question\": query})\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b3dc55b-6c4f-47d1-a52b-571de58f937d",
   "metadata": {},
   "source": [
    "This works very well actually. Latency is for sure an issue to address (NEVERMIND - Fixed the speed thing, it is good now), but the answer quality is high.\n",
    "- Idea: When prompting the final LLM, may be able to specify the MAIN question and helplful supplemental questions as something to\n",
    "        either consider or ignore as it sees fit.\n",
    "\n",
    "- For some reason, it works better when I don't do [query] + sub_queries, and only feed in sub_queries. I imagine supplying the supplemental questions as in the idea above might actually be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3801b4b-e771-499a-9982-bd0c6166aa4f",
   "metadata": {},
   "source": [
    "## Multi-Query + RAG Tree-Based Parser + Normal Retriever (Trying to Get Speed Ups - Nevermind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb89e0-487e-4f24-84c1-ddb940b9cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits_leaves, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7038ec-a057-4d94-b59c-9af02c9441b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_query_prompt = \"\"\"Break down this mathematical query into smaller, related queries that would help build a complete answer.\n",
    "Question: {question}\n",
    "\n",
    "Let's generate specific sub-queries to find:\n",
    "1. The direct definition/axioms\n",
    "2. Related properties mentioned elsewhere\n",
    "3. Any additional conditions or properties that combine with these\n",
    "4. Any special cases or extensions\n",
    "\n",
    "I need specific sub-queries. For example, given a Query: List the axioms of the reals, you should return something similar to:\n",
    "What are the field axioms that the reals satisfy?,\n",
    "What are the ordering properties of the reals?,\n",
    "What additional properties are mentioned about the real numbers?,\n",
    "How are the reals defined in terms of fields and ordering?\n",
    "\n",
    "Do not try to answer the Question at all. Just create sub-queries (so questions that help to build up a larger picture. I'm using you for a RAG tool and this is multi-query)\n",
    "Also, when you generate sub-queries, do not use your own knowledge AT ALL. Just stick to sub-queries that generalize the query based on knowledge of English and NOT of maths. Remember, this is a RAG tool.\n",
    "To re-iterate, don't use your maths knowledge AT ALL. \n",
    "Make sure to not use your maths knowledge AT ALL. \n",
    "Please don't use your maths knowledge AT ALL. \n",
    "Just don't use your maths knowledge AT ALL. \n",
    "Just don't use your own knowledge of the Question AT ALL.\n",
    "\n",
    "Importantly, you are guiding this strategy. So as the master of this RAG tool, you should 3 choose sub-queries that will be able to gather information from disparate parts of the input file.\n",
    "\n",
    "Just don't use your maths knowledge AT ALL. \n",
    "Just don't use your own knowledge of the Question AT ALL.\n",
    "\n",
    "\n",
    "Generate 3 sub-queries:\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(sub_query_prompt)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a767b-8d1e-4486-93ff-11e4fcb892b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(documents: list[list]):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"list every single definition in the context comprehensively\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53059fa-9b82-43b9-9c05-aa4cb4b13f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Mathematical Response:\n",
    "[Response here, using formal mathematical notation where appropriate]\n",
    "\n",
    "Important Notes:\n",
    "- All statements are made strictly based on the provided context\n",
    "- Any mathematical notation used follows directly from the source material\n",
    "- No additional mathematical facts or properties are assumed beyond what's given\n",
    "- Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "- Do not miss out on critical information. Make sure to include this.\n",
    "- If something is obviously not relevant to the Question, don't include it - if it is OBVIOUSLY not relevant and you can intuitively do without it\n",
    "\n",
    "[If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "    file.write(final_rag_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c11c6de6-d65e-48d8-9260-e97481efd4c3",
   "metadata": {},
   "source": [
    "This is much faster - NEVERMIND IT IS NOT, but response quality is slighty lower - might need to specify more returned documents (does improve it). Honestly though, this may just be a decent approach if speed becomes an issue - it saves on costs + the tree structure guarantees distinctness of nodes (though the retriever may get duplicates, but perhaps the union balances this) - there are a lot of duplicates though in LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8302c3-2ae7-4655-a05a-7b50d0e44ff6",
   "metadata": {},
   "source": [
    "## Helper Functions For Example Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d666013-ba1f-4401-8b1b-05f0308c012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_math_query(query: str) -> str:\n",
    "    \n",
    "    classification_prompt = f\"\"\"Determine if this mathematical query is asking for a proof or something else.\n",
    "\n",
    "    Think step by step:\n",
    "    1. Is this asking for a proof/demonstration of why something is true?\n",
    "    2. Or is it asking for something else (definition, explanation, etc.)?\n",
    "    \n",
    "    Query: {query}\n",
    "    \n",
    "    Return ONLY 'proof' or 'other'.\"\"\"\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key = os.environ['OPENAI_API_KEY']\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": classification_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip().lower()\n",
    "\n",
    "def get_top_n_distinct(documents, query, n):\n",
    "    query_embedding = embd.embed_query(query)\n",
    "    cos_sim = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    doc_with_scores = [(doc, cos_sim[i]) for i, doc in enumerate(documents)]\n",
    "    sorted_docs = sorted(doc_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in sorted_docs[:n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c539b-7a5f-4df6-bf68-341dd6248de1",
   "metadata": {},
   "source": [
    "## Simple ChatBot (Uses Multi-Query + RAG Tree-Based Parser + Routing Example + > Near-Maximum Cosine Similarity + Keeps History of Last 2 Messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d70f61-0387-4f91-9dc8-802c8fd6e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following mathematical context from the notes:\n",
    "    {context}\n",
    "    \n",
    "    Question To Answer Now: {question}\n",
    "\n",
    "    Previous Question:Response Pairs in Order from Least Recent to Most Recent (You are a ChatBot, so these are the previous Questions asked of you, and the Responses you gave to each):{q_a}\n",
    "    \n",
    "    Mathematical Response:\n",
    "    [Response here, using formal mathematical notation where appropriate]\n",
    "    \n",
    "    Important Notes:\n",
    "    - All statements are made strictly based on the provided context\n",
    "    - Any mathematical notation used follows directly from the source material\n",
    "    - No additional mathematical facts or properties are assumed beyond what's given\n",
    "    - Where multiple pieces of context overlap, they may be unified into a coherent response while preserving mathematical rigor. Only do this if it is logical and is appropriate/makes sense.\n",
    "    - Do not miss out on critical information. Make sure to include this.\n",
    "    - You may or may not need to incorporate the previous Questions and Responses. Do this when you think it is suitable.\n",
    "    \n",
    "    [If any crucial context appears to be missing for a complete answer, note that explicitly]\n",
    "    \"\"\"\n",
    "\n",
    "q_a_list = []\n",
    "\n",
    "while True:\n",
    "    distinct_docs = []\n",
    "    query = input('Query: ')\n",
    "    q_a = 'No Previous Questions and Responses'\n",
    "    if len(q_a_list) != 0:\n",
    "        q_a = '\\n' + ''.join(q_a_list)\n",
    "    query_template = f\"\"\"The Current Query is: {query}.\n",
    "    The previous Questions and Responses in order from Least Recent to Most Recent (You are a helper tool to a Chatbot, so these are the previous Questions asked of it, and the Responses it gave to each):{q_a}\n",
    "    \"\"\"\n",
    "    query_type = classify_math_query(query)\n",
    "    if query_type == 'other':\n",
    "        sub_queries = generate_sub_queries(query_template)\n",
    "        while len(sub_queries) != 3:\n",
    "            sub_queries = generate_sub_queries(query_template)\n",
    "        \n",
    "        for sq in [query] + sub_queries:\n",
    "            distinct_docs.append(get_sorted_documents_without_neighbors(splits_leaves, sq))\n",
    "            \n",
    "        seen_content = set()\n",
    "        unique_docs = []\n",
    "        for doc in [item for sublist in distinct_docs for item in sublist]:\n",
    "            if doc.page_content not in seen_content:\n",
    "                seen_content.add(doc.page_content)\n",
    "                unique_docs.append(doc)\n",
    "    else:\n",
    "        unique_docs = get_top_n_distinct(splits_leaves, query, 1)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "    \n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    with open('final.txt', 'w', encoding = 'utf-8') as file:\n",
    "        q_a = 'No Previous Questions and Responses'\n",
    "        if len(q_a_list) != 0:\n",
    "            q_a = '\\n' + ''.join(q_a_list)\n",
    "        response = rag_chain.invoke({\"context\": unique_docs,\"question\": query, \"q_a\": q_a})\n",
    "        file.write(response)\n",
    "        if len(q_a_list) < 2:\n",
    "            q_a_list.append('Question: ' + query + ', Response: ' + response + '\\n')\n",
    "        else:\n",
    "            q_a_list.pop(0)\n",
    "            q_a_list.append('Question: ' + query + ', Response: ' + response + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
